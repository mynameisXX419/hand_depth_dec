# ============================================
# pressget_dual_v7_detector_merge10ms.py
# â€”â€” å®æ—¶å‹åŠ›æ£€æµ‹ï¼ˆèåˆç‰ˆï¼š10mså³°å€¼åˆå¹¶ + å®æ—¶ç›‘å¬ + å…¨å±€è®¡æ•°ï¼‰
# ============================================

import pandas as pd
import numpy as np
import os, time, json
from scipy.signal import find_peaks
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# ---------- å‚æ•°é…ç½® ----------
FILE_PATH = "/home/ljy/project/hand_dec/ljy/ljy_1/pressure_log.csv"
DIR_PATH  = os.path.dirname(FILE_PATH)
SAVE_FILE = "/home/ljy/project/hand_dec/datacap/1/pressure_peaks_valid.csv"
COUNT_FILE = "/home/ljy/project/hand_dec/datacap/1/press_count.json"

STATIC_STD_THRESH = 1000.0        # é™æ­¢æ®µåˆ¤æ–­é˜ˆå€¼
VALID_PRESS_THRESH = 2000         # âœ… æœ‰æ•ˆæŒ‰å‹æ­£å³°é˜ˆå€¼
MIN_DISTANCE = 1                  # å³°å€¼é—´æœ€å°é—´è·ï¼ˆæ ·æœ¬ç‚¹æ•°ï¼‰
PROMINENCE = 20                   # å³°å€¼æ˜¾è‘—æ€§è¦æ±‚
MERGE_WINDOW_MS = 10              # âœ… 10ms å†…åªä¿ç•™æœ€å¤§å³°å€¼
DELAY_AFTER_WRITE = 0.1           # C++ å†™å…¥å»¶è¿Ÿè¡¥å¿

# ---------- å…¨å±€å˜é‡ ----------
last_peak_time = 0
total_valid_count = 0
session_count = 0


# ---------- è¾…åŠ©å‡½æ•° ----------
def load_press_count():
    """ä» JSON æ–‡ä»¶æ¢å¤ç´¯è®¡æ¬¡æ•°"""
    global total_valid_count
    if os.path.exists(COUNT_FILE):
        try:
            with open(COUNT_FILE, "r") as f:
                data = json.load(f)
                total_valid_count = int(data.get("total_valid_count", 0))
            print(f"[INFO] å·²æ¢å¤ç´¯è®¡æŒ‰å‹æ¬¡æ•°: {total_valid_count}")
        except Exception:
            print("[WARN] è¯»å–ç´¯è®¡è®¡æ•°æ–‡ä»¶å¤±è´¥ï¼Œé‡æ–°è®¡æ•°ã€‚")
            total_valid_count = 0
    else:
        total_valid_count = 0


def save_press_count():
    """ä¿å­˜ç´¯è®¡æ¬¡æ•°åˆ° JSON æ–‡ä»¶"""
    with open(COUNT_FILE, "w") as f:
        json.dump({"total_valid_count": total_valid_count}, f)


# ---------- æ ¸å¿ƒæ£€æµ‹å‡½æ•° ----------
def detect_global_valid_peaks(file_path: str):
    """èåˆç‰ˆæ ¸å¿ƒé€»è¾‘ï¼š10mså†…å³°å€¼åˆå¹¶ + æ–°å³°æ£€æµ‹ + æ–‡ä»¶ä¿å­˜"""
    global last_peak_time, total_valid_count, session_count

    if not os.path.exists(file_path):
        return

    try:
        df = pd.read_csv(file_path, usecols=["time_ms", "press_sum_norm"])
    except Exception as e:
        print(f"[ERROR] æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return

    if df.empty or len(df) < 10:
        return

    df["press_sum_norm"] = pd.to_numeric(df["press_sum_norm"], errors="coerce")
    df = df.dropna(subset=["press_sum_norm"])

    time_ms = df["time_ms"].to_numpy(dtype=float)
    press   = df["press_sum_norm"].to_numpy(dtype=float)

    # ---- é™æ­¢æ®µåˆ¤æ–­ ----
    std_val = np.std(press[-50:])
    if std_val < STATIC_STD_THRESH:
        print(f"ğŸŸ¢ é™æ­¢æ®µ (STD={std_val:.1f})")
        return

    # ---- åˆæ­¥å³°å€¼æ£€æµ‹ ----
    pos_locs, _ = find_peaks(press, prominence=PROMINENCE, distance=MIN_DISTANCE)
    if len(pos_locs) == 0:
        return

    peaks_time = time_ms[pos_locs]
    peaks_val  = press[pos_locs]

    # ---- åˆå¹¶10mså†…çš„è¿‘é‚»å³°ï¼Œåªä¿ç•™æœ€å¤§å€¼ ----
    merged_times, merged_vals = [], []
    if len(peaks_time) > 0:
        group_start = 0
        for i in range(1, len(peaks_time)):
            if peaks_time[i] - peaks_time[i - 1] <= MERGE_WINDOW_MS:
                continue
            else:
                group_slice = slice(group_start, i)
                max_idx = np.argmax(peaks_val[group_slice]) + group_start
                merged_times.append(peaks_time[max_idx])
                merged_vals.append(peaks_val[max_idx])
                group_start = i
        group_slice = slice(group_start, len(peaks_time))
        max_idx = np.argmax(peaks_val[group_slice]) + group_start
        merged_times.append(peaks_time[max_idx])
        merged_vals.append(peaks_val[max_idx])

    peaks_time = np.array(merged_times)
    peaks_val  = np.array(merged_vals)

    # ---- ç­›é€‰æœ‰æ•ˆæŒ‰å‹ ----
    valid_mask = peaks_val > VALID_PRESS_THRESH
    peaks_time = peaks_time[valid_mask]
    peaks_val  = peaks_val[valid_mask]
    if len(peaks_time) == 0:
        return

    # ---- æ–°å³°å€¼è¿‡æ»¤ ----
    new_idx = peaks_time > last_peak_time
    if not np.any(new_idx):
        return

    new_times = peaks_time[new_idx]
    new_vals  = peaks_val[new_idx]

    # ---- å†™å…¥æ–‡ä»¶ + æ›´æ–°ç¼–å· ----
    press_ids_global = [total_valid_count + i + 1 for i in range(len(new_times))]
    df_valid = pd.DataFrame({
        "press_id_global": press_ids_global,
        "t_pos_ms": new_times,
        "press_pos": new_vals
    })

    df_valid.to_csv(SAVE_FILE, mode='a', header=not os.path.exists(SAVE_FILE), index=False)

    total_valid_count += len(df_valid)
    session_start_id = session_count + 1
    session_count += len(df_valid)
    last_peak_time = new_times[-1]
    save_press_count()

    for i, row in enumerate(df_valid.itertuples(), start=session_start_id):
        print(f"âœ… ç¬¬ {i} æ¬¡æœ‰æ•ˆæŒ‰å‹: {row.press_pos:.0f} @ {row.t_pos_ms:.0f} ms")


# ---------- æ–‡ä»¶ç›‘å¬ ----------
class PressureWatcher(FileSystemEventHandler):
    """ç›‘å¬ pressure_log.csv æ–‡ä»¶å˜åŒ–å¹¶è§¦å‘æ£€æµ‹"""
    def on_modified(self, event):
        if not event.src_path.endswith("pressure_log.csv"):
            return
        time.sleep(DELAY_AFTER_WRITE)
        detect_global_valid_peaks(FILE_PATH)


# ---------- ä¸»ç¨‹åº ----------
if __name__ == "__main__":
    print(f"[INFO] æ­£åœ¨ç›‘å¬æ–‡ä»¶å˜åŒ–: {FILE_PATH}")
    if not os.path.exists(FILE_PATH):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {FILE_PATH}")
        exit(1)

    load_press_count()

    event_handler = PressureWatcher()
    observer = Observer()
    observer.schedule(event_handler, DIR_PATH, recursive=False)
    observer.start()
    print("[INFO] æ–‡ä»¶ç›‘å¬å·²å¯åŠ¨ï¼Œç­‰å¾… C++ å†™å…¥ä¸­...\n")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
        print(f"\n[INFO] æ‰‹åŠ¨é€€å‡ºç›‘å¬ã€‚å½“å‰ä¼šè¯æ£€æµ‹åˆ° {session_count} æ¬¡æœ‰æ•ˆæŒ‰å‹ï¼Œæ€»ç´¯è®¡ {total_valid_count} æ¬¡ã€‚")
        save_press_count()

    observer.join()
